{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "62882f0d-8214-4bf8-8fde-c7bbd5e7cca7",
   "metadata": {},
   "source": [
    "# Preprocessing Pipeline for Structural Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279bc4b0-9a12-4949-8bec-b38bb68466bc",
   "metadata": {},
   "source": [
    "- Sections:\n",
    "    - Set-up: Run every time to create input dictionary\n",
    "    - Part 1:\n",
    "    - Part 2:\n",
    "    - Part 3:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ae7824-79ee-48a2-a1a5-8239f63379f0",
   "metadata": {},
   "source": [
    "# Set-Up: \n",
    "## Select Subjects to Process and Set Variables - This section needs to be run before each of the 3 Parts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1faf96b-6ceb-4387-9eb5-1048b2b086fe",
   "metadata": {},
   "source": [
    "### Packages and Functions Needed for This Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "2d6d7766-fe1c-4e5b-b9e5-b1e352fdbf6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from glob import glob\n",
    "from nilearn import plotting\n",
    "import subprocess\n",
    "import sys\n",
    "import matplotlib_inline\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "from pathlib import Path\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# export MPLBACKEND=TkAgg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "38040f25-9805-448a-aad4-a4a927c212bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_slurm_job(job_name, command, partition=\"bch-compute\", nodes=1, ntasks=1, cpus=16, mem=\"50G\", time=\"24:00:00\"):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Submits a job to the Slurm job scheduler.\n",
    "\n",
    "    This function creates a script that has the given command, and then submits this script to the Slurm job scheduler.\n",
    "    \n",
    "    Parameters:\n",
    "    job_name (str): A name for the job. This will help you identify the job later.\n",
    "    command (str): The command that you want to run.\n",
    "    partition (str): The partition on the cluster where you want to run the job. Defaults to \"bch-compute\".\n",
    "    nodes (int): The number of nodes (computers) that you want to use to run the job. Defaults to 1.\n",
    "    cpus_per_task (int): The number of CPUs (processing units) that you want to use on each node. Defaults to 16.\n",
    "    mem (str): The amount of memory that you want to use on each node. Defaults to \"50GB\".\n",
    "    time (str): The maximum amount of time that the job is allowed to run. Defaults to \"10:00:00\" (10 hours).\n",
    "\n",
    "    Returns:\n",
    "    job_id (str): The ID of the job that was submitted. You can use this ID to check on the status of the job later.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    script = f\"\"\"#!/bin/bash\n",
    "    \n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --partition={partition}\n",
    "#SBATCH --nodes={nodes}\n",
    "#SBATCH --ntasks={ntasks} \n",
    "#SBATCH --cpus-per-task={cpus}\n",
    "#SBATCH --mem={mem}\n",
    "#SBATCH --time={time}\n",
    "#SBATCH -o output_%j.txt\n",
    "#SBATCH --mail-type=NONE\n",
    "\n",
    "    # Run the command\n",
    "    export MPLBACKEND=TkAgg\n",
    "    \n",
    "    source /lab-share/Neuro-Cohen-e2/Public/environment/load_neuroimaging_env.sh\n",
    "\n",
    "    set -e\n",
    "\n",
    "    {command}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write the script to a temporary file\n",
    "    with tempfile.NamedTemporaryFile(mode='w', delete=False) as f:\n",
    "        f.write(script)\n",
    "        script_file = f.name\n",
    "\n",
    "    # Make the script executable\n",
    "    subprocess.run([\"chmod\", \"+x\", script_file])\n",
    "\n",
    "    try:\n",
    "        # Submit the job using sbatch through the shell\n",
    "        output = subprocess.check_output(['sbatch', script_file]).decode('utf-8')\n",
    "\n",
    "        # Extract the job ID from the output\n",
    "        job_id = output.strip().split()[-1]\n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Error submitting job: {e}\")\n",
    "        job_id = None\n",
    "    \n",
    "    sleep(5)\n",
    "\n",
    "    return job_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "89dcc226-309c-40e4-bbec-6b3f160591e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def submit_slurm_job_test(job_name, command, partition=\"bch-compute\", nodes=1, ntasks=1, cpus=16, mem=\"50G\", time=\"24:00:00\"):\n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    Submits a job to the Slurm job scheduler.\n",
    "\n",
    "    This function creates a script that has the given command, and then submits this script to the Slurm job scheduler.\n",
    "    \n",
    "    Parameters:\n",
    "    job_name (str): A name for the job. This will help you identify the job later.\n",
    "    command (str): The command that you want to run.\n",
    "    partition (str): The partition on the cluster where you want to run the job. Defaults to \"bch-compute\".\n",
    "    nodes (int): The number of nodes (computers) that you want to use to run the job. Defaults to 1.\n",
    "    cpus_per_task (int): The number of CPUs (processing units) that you want to use on each node. Defaults to 16.\n",
    "    mem (str): The amount of memory that you want to use on each node. Defaults to \"50GB\".\n",
    "    time (str): The maximum amount of time that the job is allowed to run. Defaults to \"10:00:00\" (10 hours).\n",
    "\n",
    "    Returns:\n",
    "    job_id (str): The ID of the job that was submitted. You can use this ID to check on the status of the job later.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    script = f\"\"\"#!/bin/bash\n",
    "    \n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --partition={partition}\n",
    "#SBATCH --nodes={nodes}\n",
    "#SBATCH --ntasks={ntasks}  \n",
    "#SBATCH --cpus-per-task={cpus}\n",
    "#SBATCH --mem={mem}\n",
    "#SBATCH --time={time}\n",
    "#SBATCH -o output_%j.txt\n",
    "#SBATCH --mail-type=NONE\n",
    "\n",
    "    # Run the command\n",
    "    export MPLBACKEND=TkAgg\n",
    "    source /lab-share/Neuro-Cohen-e2/Public/environment/load_neuroimaging_env.sh\n",
    "\n",
    "    set -e\n",
    "\n",
    "    {command}\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write the script to a temporary file\n",
    "    with open(f'slurm_script_{job_name}.sh', 'w') as f:\n",
    "        f.write(script)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "6424c527-7a6b-4f6e-9c92-cc1c4389e281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_input_dict(input_folder, subjects_to_skip=None, input_type='Folder'):\n",
    "    \n",
    "    \"\"\"\n",
    "    Creates a dictionary of subjects and their corresponding sessions from a given input folder.\n",
    "\n",
    "    This function looks at the files in the input folder and creates a dictionary where the keys are the subject IDs and the values are lists of sessions for each subject.\n",
    "\n",
    "    Parameters:\n",
    "    input_folder (str): The path to the folder that contains the input files.\n",
    "    subjects_to_skip (list): A list of subject IDs that you want to skip. Defaults to None.\n",
    "    input_type (str): The type of input folder. Can be either 'BIDS' or 'Folder'. Defaults to 'Folder'.\n",
    "\n",
    "    Returns:\n",
    "    subject_sessions (dict): A dictionary where the keys are the subject IDs and the values are lists of sessions for each subject.\n",
    "\n",
    "    Note:\n",
    "    If input_type is 'BIDS', the function assumes that the input folder is organized according to the BIDS format.\n",
    "    If input_type is 'Folder', the function assumes that all selected scans are in one folder and the file names start with the subject ID followed by an underscore.\n",
    "    \"\"\"\n",
    "        \n",
    "    \n",
    "    if not Path(input_folder).is_dir():\n",
    "        raise ValueError(\"Input folder is not a valid directory\")\n",
    "    \n",
    "    subject_sessions = {}\n",
    "    \n",
    "    if input_type=='BIDS':\n",
    "        subjects=[f for f in os.listdir(input_folder) if os.path.isdir(os.path.join(input_folder, f))]\n",
    "        \n",
    "        if subjects_to_skip is not None:\n",
    "            subjects = [subject for subject in subjects if subject not in subjects_to_skip]\n",
    "            \n",
    "        print(\"There are\", len(subjects), \"unique subjects to be registered\")\n",
    "        \n",
    "        for subject in sorted(subjects):\n",
    "            # Get the path to the subject folder\n",
    "            subject_path = os.path.join(input_folder, subject)\n",
    "\n",
    "            # Get a list of session folders within the subject folder\n",
    "            sessions = [f for f in os.listdir(subject_path) if os.path.isdir(os.path.join(subject_path, f))]\n",
    "\n",
    "            # Add the subject and sessions to the dictionary\n",
    "            subject_sessions[subject] = sessions\n",
    "\n",
    "\n",
    "        print(subject_sessions)\n",
    "        \n",
    "    \n",
    "    elif input_type=='Folder':\n",
    "        #All selected scans are in one folder\n",
    "        #Assumptions: first part of file name is subject ID followed by an _\n",
    "        subjects=sorted(set([os.path.basename(i).split('_')[0] for i in glob(f'{input_folder}/*.nii*')]))\n",
    "        \n",
    "        if subjects_to_skip is not None:\n",
    "            subjects = [subject for subject in subjects if subject not in subjects_to_skip]\n",
    "            \n",
    "        print(\"There are\", len(subjects), \"unique subjects to be registered\")\n",
    "        \n",
    "        \n",
    "        for subject in sorted(subjects):\n",
    "            for file in glob(f'{input_folder}/*{subject}*.nii*'):\n",
    "                subject = os.path.basename(file).split('_')[0]\n",
    "                session = os.path.basename(file).split('_')[1]\n",
    "\n",
    "                if subject not in subject_sessions:\n",
    "                    subject_sessions[subject] = []\n",
    "                if session not in subject_sessions[subject]:  \n",
    "                    subject_sessions[subject].append(session)\n",
    "\n",
    "\n",
    "        print(subject_sessions)\n",
    "\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Invalid input_type: '{input_type}'. Should be either 'BIDS' or 'Folder'.\")\n",
    "    \n",
    "    \n",
    "    return subject_sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "eeb2392c-8678-49cf-b9b8-d6fb511398cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_registration_target(file_names):\n",
    "    \n",
    "    \"\"\"\n",
    "    Sets the registration target based on a list of file names.\n",
    "\n",
    "    This function looks at the file names and sets the registration target to the first file that matches a certain criterion.\n",
    "\n",
    "    Parameters:\n",
    "    file_names (list): A list of file names.\n",
    "\n",
    "    Returns:\n",
    "    reg_target (str): The registration target.\n",
    "    \n",
    "    Note:\n",
    "    The function checks if any of the file names contain the registration target strings (Reg_target_1 or Reg_target_2).\n",
    "    If no registration target is found, a ValueError is raised.\n",
    "    \"\"\"\n",
    "        \n",
    "    global Reg_target_1\n",
    "    global Reg_target_2\n",
    "    reg_target = None\n",
    "    for file_name in file_names:\n",
    "        if Reg_target_1 in file_name:\n",
    "            reg_target = Reg_target_1\n",
    "            break\n",
    "    if reg_target is None:\n",
    "        for file_name in file_names:\n",
    "            if Reg_target_2 in file_name:\n",
    "                reg_target = Reg_target_2\n",
    "                break\n",
    "    if reg_target is None:\n",
    "        raise ValueError(f\"No registration target found in {file_names}\")\n",
    "    return reg_target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56726836-88e5-469b-9a2b-92b794a833bc",
   "metadata": {},
   "source": [
    "### Put your information below then create the input dictionary (subject_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "e1a65641-f5c9-4b4b-a3e8-f6b7036dc0a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# input_dir=\"test_input_combine\"  #Folder with input files\n",
    "# #input_dir = '/lab-share/Neuro-Cohen-e2/Public/lesions/RDCRN_TSC/'\n",
    "\n",
    "# input_type='BIDS' #'BIDS' or 'Folder'\n",
    "# subjects_to_skip=['sub-MGH083']  \n",
    "\n",
    "# output_dir=\"output_combine_test\"\n",
    "\n",
    "\n",
    "# IMAGE_TYPES = ['T1w', 'T2w', 'FLAIR'] #case sensitive, change to match what you used e.g. t1, t1w, TI \n",
    "# Reg_target_1='T1w' #your ideal registration target, case sensitive\n",
    "# Reg_target_2='T2w' #your second choice for registration target, case sensitive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "4460d857-8920-4d95-94fc-b68711883cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir=\"RDCRN_test\"  #Folder with input files\n",
    "input_type='BIDS' #'BIDS' or 'Folder'\n",
    "#input_dir = '/lab-share/Neuro-Cohen-e2/Public/lesions/RDCRN_TSC/'\n",
    "output_dir=\"/lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619\"\n",
    "\n",
    "subjects_to_skip=None\n",
    "\n",
    "skullstrip='synthstrip' \n",
    "\n",
    "mni_software='EasyReg'\n",
    "\n",
    "IMAGE_TYPES = ['t1', 't2'] #case sensitive, change to match what you used e.g. t1, t1w, TI \n",
    "Reg_target_1='t1' #your ideal registration target, case sensitive\n",
    "Reg_target_2='t2' #your second choice for registration target, case sensitive\n",
    "\n",
    "#Bias correction only works on T1 or T2, bias correction is set to work on the registration target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "de8ecb37-aa96-4360-920c-69fef15181ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir=\"/lab-share/Neuro-Cohen-e2/Public/notebooks/jpeters/RAW/\"  #Folder with input files\n",
    "input_type='Folder' #'BIDS' or 'Folder'\n",
    "output_dir=\"output_jpeters_20240620_test\"\n",
    "\n",
    "subjects_to_skip=['c002', 'c003'] \n",
    "\n",
    "skullstrip='both' #synthstrip, optibet, both, None \n",
    "                        #for both - run it for Part2 (on registration target) and pick your favorite, then update this variable before running Part3\n",
    "mni_software='EasyReg' #'ANTs' or 'ANTsQuick'\n",
    "\n",
    "IMAGE_TYPES = ['t1', 'T2', 'FLAIR'] #case sensitive, change to match what you used e.g. t1, t1w, TI \n",
    "Reg_target_1='t1' #your ideal registration target, case sensitive\n",
    "Reg_target_2='T2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "e2a9578b-4349-4bc9-a342-fec1ed3416c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 unique subjects to be registered\n",
      "{'c001': ['s01']}\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary of subjects and their corresponding sessions from the input directory\n",
    "subject_sessions=create_input_dict(input_dir, subjects_to_skip=subjects_to_skip, input_type=input_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824b32ef-fcee-458d-bf74-e9c7ca485dba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Part 1: Make Output Folders and Combine Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7215de6a-5625-4741-aba0-db34364954c1",
   "metadata": {},
   "source": [
    "## Part 1 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "682ba1f0-c3c3-4621-b253-12f64dc74e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_tree(d, n=5, indent=0):\n",
    "    \"\"\"\n",
    "    Recursively prints the folder structure.\n",
    "    \n",
    "    Parameters:\n",
    "    d (dict): The folder structure dictionary.\n",
    "    n (int): The maximum number of subjects to print. Default is 5.\n",
    "    indent (int): The indentation level (number of spaces). Default is 0.\n",
    "    \"\"\"\n",
    "    \n",
    "    subset = {k: d[k] for k in list(d)[:n]}\n",
    "    \n",
    "    for key, value in subset.items():\n",
    "        print('    ' * indent + str(key))\n",
    "        if isinstance(value, list):\n",
    "            for item in value:\n",
    "                print('    ' * (indent + 1) + str(item))\n",
    "        elif isinstance(value, dict):\n",
    "            print_tree(value, indent + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "5546dff2-6cd4-4d1a-a359-ff5e4cd4ad48",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def combine_images(working_dir, list_of_images, out_name, clean_up=True):\n",
    "#images should be inside working_dir somewhere \n",
    "\n",
    "#def combine_images(working_dir, input_dir, participant, session, image_type, list_of_images, clean_up=True):\n",
    "    \"\"\"\n",
    "    Combines images of different directions using niftymic.\n",
    "    \n",
    "    Parameters:\n",
    "    working_dir (str): The working directory.\n",
    "    input_dir (str): The input directory.\n",
    "    participant (str): The participant ID.\n",
    "    session (str): The session ID.\n",
    "    image_type (str): The image type.\n",
    "    list_of_images (list): The list of images to combine.\n",
    "    clean_up (bool): Whether to clean up temporary files. Default is True.\n",
    "    \n",
    "    Returns:\n",
    "    command (str): The command to combine images.\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    BIDSPATH = '/lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/0_pipeline_scripts'\n",
    "   \n",
    "    \n",
    "    for i, image in enumerate(list_of_images, start=1):\n",
    "        mask_file = f'{working_dir}/temp_{i}_{out_name}_mask.nii.gz'\n",
    "        \n",
    "\n",
    "        result = subprocess.run(['fslmaths', image, '-abs', '-bin', mask_file], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        if result.returncode != 0:\n",
    "            print(result.stderr.decode())\n",
    "            raise Exception(f'Failed to create mask for {image}')\n",
    "    \n",
    "    mask_files = [f'{working_dir}/temp_{i}_{out_name}_mask.nii.gz' for i in range(1, len(list_of_images) + 1)]\n",
    "    output_file = f'{working_dir}/{out_name}.nii.gz'\n",
    "    #images=[i.replace(f'{working_dir}', '/app/data') for i in list_of_images] \n",
    "    \n",
    "    cmd = [\n",
    "        'singularity', 'exec',\n",
    "        '-B', f'{working_dir}:/app/data',\n",
    "        '-B', f'{BIDSPATH}:{BIDSPATH}',\n",
    "        f'{BIDSPATH}/niftymic.sif',\n",
    "        'niftymic_reconstruct_volume',\n",
    "        '--filenames', *list_of_images,\n",
    "        '--filenames-masks', *mask_files,\n",
    "        '--output', output_file\n",
    "    ]\n",
    "    \n",
    "    if clean_up == True:\n",
    "        cmd += '\\n'\n",
    "        cmd += [\n",
    "            'rm', '-r', \n",
    "            f'{working_dir}/config*', \n",
    "            f'{working_dir}/temp*', \n",
    "            f'{working_dir}/*mask*', \n",
    "            f'{working_dir}/motion_correction'\n",
    "        ]\n",
    "        \n",
    "    command = ' '.join(cmd)\n",
    "\n",
    "    return command\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db710cf2-b69d-4fe0-8716-d7b874568c1c",
   "metadata": {},
   "source": [
    "### Part 1a : Make output folder system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "361fe04c-9d1c-4b4f-ba7e-5f7c32da5bd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your output folder, output_jpeters_20240620_test, will look something like this: \n",
      "c001\n",
      "    s01\n"
     ]
    }
   ],
   "source": [
    "print(f'Your output folder, {output_dir}, will look something like this: ')\n",
    "print_tree(subject_sessions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1978c62c-df4a-4594-bb8c-1f2687d999b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing output folders for s01 for c001\n"
     ]
    }
   ],
   "source": [
    "# Create output folders for each subject and session based on the subject_sessions dictionary\n",
    "for subject, sessions in subject_sessions.items():\n",
    "    \n",
    "    subject_folder = os.path.join(output_dir, subject)\n",
    "    \n",
    "    # Create the subject folder if it doesn't exist\n",
    "    if not os.path.exists(subject_folder):\n",
    "        os.makedirs(subject_folder)\n",
    "    \n",
    "    \n",
    "    # Loop through each session for the subject\n",
    "    for session in sorted(set(sessions)):\n",
    "        print(f'Preparing output folders for {session} for {subject}')\n",
    "        session_folder = os.path.join(subject_folder, session)\n",
    "         \n",
    "        # Check if the session folder exists and is not empty\n",
    "        if os.path.exists(session_folder):\n",
    "            if os.listdir(session_folder):\n",
    "                print(f\"{subject}: {session_folder} exists and is not empty\")\n",
    "        else:\n",
    "            # Create the session folder if it doesn't exist\n",
    "            if not os.path.exists(session_folder):\n",
    "                os.makedirs(session_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872969d8-edc1-41c5-b968-e8fcea9565f6",
   "metadata": {},
   "source": [
    "### Part 1b: copy selected files to output folder system, combine images as needed (will submit to SLURM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be069d-7e72-4f49-b85d-197dd5ea841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy images to output folders; Combine images as needed\n",
    "combine_dict={}\n",
    "for subject, sessions in subject_sessions.items():\n",
    "    subject_folder = os.path.join(output_dir, subject)\n",
    "    \n",
    "\n",
    "    # Loop through each session for the subject\n",
    "    for session in sorted(set(sessions)):\n",
    "        session_folder = os.path.join(subject_folder, session)\n",
    "        \n",
    "        if not os.path.exists(f'{session_folder}/SELECTED'):\n",
    "            os.makedirs(f'{session_folder}/SELECTED')\n",
    "         \n",
    "        # Loop through each image type \n",
    "        for image_type in IMAGE_TYPES:\n",
    "\n",
    "            # Check if a file with the image type already exists in the session folder\n",
    "            if glob(f'{session_folder}/SELECTED/*{image_type}*.nii*'):\n",
    "                    print(f\"Error: A file with image type '{image_type}' already exists in {session_folder}/SELECTED\")\n",
    "            else: \n",
    "                # Get the list of images for the image type\n",
    "                if input_type == 'Folder':\n",
    "                    images = glob(f'{input_dir}/{subject}*{session}*{image_type}*.nii*') \n",
    "                elif input_type == 'BIDS':\n",
    "                    images = glob(f'{input_dir}/{subject}/{session}/*{image_type}*.nii*')\n",
    "             \n",
    "                # Check if there are more than 3 images to combine\n",
    "                if len(images) > 3:\n",
    "                    print(f\"Error: More than 3 images found for participant {subject}, session {session}, and image type {image_type}.\")\n",
    "                    continue\n",
    "                \n",
    "                # Combine the images if there are more than 1\n",
    "                if len(images) > 1 and image_type not in ['FLAIR', 'flair', 'Flair']:\n",
    "                    os.makedirs(f'{session_folder}/SELECTED/precombine', exist_ok=True)\n",
    "                    [shutil.copy(i,f'{session_folder}/SELECTED/precombine') for i in images]\n",
    "                    print(f\"combining: {images}\")\n",
    "                    images_to_combine=glob(f'{session_folder}/SELECTED/precombine/*{image_type}*nii*') \n",
    "                    command = combine_images(f'{session_folder}/SELECTED', images_to_combine, f'{subject}_{session}_{image_type}', clean_up=True)\n",
    "                    job_name = f\"combine_images_{subject}_{session}_{image_type}\"\n",
    "                    job_id=submit_slurm_job(job_name, command)\n",
    "                    combine_dict[(subject,session)] = job_id\n",
    "                    \n",
    "                if len(images) > 1 and image_type in ['FLAIR', 'flair', 'Flair']:\n",
    "                    print('You have more than one FLAIR image, Combining these may not work well, please select one')\n",
    "                \n",
    "                # Copy the single image to the session folder\n",
    "                elif len(images) == 1:\n",
    "                    print(f\"copying: {images} to {session_folder}/SELECTED/{subject}_{session}_{image_type}.nii.gz\")\n",
    "                    shutil.copy(images[0], f'{session_folder}/SELECTED/{subject}_{session}_{image_type}.nii.gz')\n",
    "\n",
    "# Print the number of combine jobs submitted to SLURM\n",
    "if len(combine_dict) > 0:\n",
    "    print('You have', len(combine_dict), 'Combine Jobs submitted to SLURM; subject and job IDs are stored in combine_dict')\n",
    "    print('You can type \"squeue -u $USER\" into your terminal to track SLURM job progress')\n",
    "    print('You can check the output file matching the jobid in combine_dict to see code outputs and any errors')\n",
    "else:\n",
    "    print('You have', len(combine_dict), 'Combine Jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "45806a5e-d588-4ea2-b5a1-d418da51a00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           1590972 bch-compu combine_ ch236393  R       0:18      1 compute-10-15\n",
      "           1587644 bch-inter     bash ch236393  R    4:05:01      1 compute-5-0-3\n"
     ]
    }
   ],
   "source": [
    "#See your combine jobs on SLURM\n",
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b33f006-7099-4ed1-9948-52db4d1bc3a2",
   "metadata": {},
   "source": [
    "# Part 2: Prepare Registration Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3417a52d-5ed3-4f8b-bf9d-3df2bf9dbb83",
   "metadata": {},
   "source": [
    "## Part 2 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3f88e9d9-f100-42dd-8c97-e7df25cd48f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reslice_image(input_image):\n",
    "    \"\"\"\n",
    "    Reslices an image to 1mm isovolumetric if the largest pixel dimension is greater than 1.5mm.\n",
    "    \n",
    "    Parameters:\n",
    "    input_folder (str): The input folder.\n",
    "    participant (str): The participant ID.\n",
    "    session (str): The session ID.\n",
    "    reg_target (str): The registration target.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(input_image):\n",
    "        raise FileNotFoundError(f\"File {input_image} does not exist\")\n",
    "    # Get the maximum pixel width\n",
    "    cmd = f\"fslinfo {input_image} | grep pixdim[1-3] | awk '{{ print $2 }}' | sort -rn | head -1\"\n",
    "    max_pixelwidth = float(subprocess.check_output(cmd, shell=True).strip())\n",
    "\n",
    "    if max_pixelwidth > 1.5:\n",
    "        print(f\"Largest pixel dimension is {max_pixelwidth} > 1.5mm, reslicing to 1mm isovolumetric\")\n",
    "        \n",
    "        stem = input_image.split('.')[0]\n",
    "        size = 1\n",
    "        output_file = f\"{stem}_{size}mm.nii.gz\"\n",
    "\n",
    "        cmd = f\"flirt -interp spline -in {input_image} -ref {input_image} -applyisoxfm {size} -out {output_file}\"\n",
    "        subprocess.run(cmd, shell=True)\n",
    "\n",
    "        os.rename(input_image, f\"{stem}_aniso.nii.gz\")\n",
    "        os.rename(output_file, input_image)\n",
    "    else:\n",
    "        print(f\"Largest pixel dimension is {max_pixelwidth}, leaving image alone\")\n",
    "        \n",
    "\n",
    "def bias_corr(input_image, image_type, skullstrip=None, clean_up=True):\n",
    "    \"\"\"\n",
    "    Performs bias correction on an image.\n",
    "    \n",
    "    Parameters:\n",
    "    input_folder (str): The input folder.\n",
    "    participant (str): The participant ID.\n",
    "    session (str): The session ID.\n",
    "    reg_target (str): The registration target.\n",
    "    mask (bool): Whether to create a brain mask. Default is True.\n",
    "    clean_up (bool): Whether to clean up temporary files. Default is True.\n",
    "    \n",
    "    Returns:\n",
    "    cmd (str): The command to run.\n",
    "    \"\"\"\n",
    "        \n",
    "    stem = input_image.split('.')[0] \n",
    "    folder = os.path.dirname(input_image)\n",
    "    cmd=\"echo Starting\\n\"\n",
    "    if os.path.exists(f\"{stem}_orig.nii.gz\"):\n",
    "        print(f'{stem}_orig.nii.gz already exists, suggesting this image has been bias corrected already!')\n",
    "        return\n",
    "    \n",
    "    # add scripts to Path so code can find them\n",
    "    cmd = f\"export PATH=$PATH:/lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/0_pipeline_scripts/\\n\"\n",
    "    \n",
    "    # Run fsl_anat_alt.sh\n",
    "    cmd += f\"fsl_anat_alt.sh -i {stem} -t {img_type} --noreg --nosubcortseg --noseg\\n\"\n",
    "\n",
    "    # Rename files\n",
    "    cmd += f\"mv {stem}.nii.gz {stem}_orig.nii.gz\\n\" \n",
    "    cmd += f\"mv {stem}.anat/T1_biascorr.nii.gz {stem}.nii.gz\\n\" \n",
    "\n",
    "    \n",
    "    if skullstrip in ['optibet', 'both']:\n",
    "        suffix = '_optibet'\n",
    "        cmd += f\"mv {stem}.anat/{image_type}_biascorr_brain.nii.gz {stem}_SkullStripped{suffix}.nii.gz\\n\"\n",
    "        cmd += f\"mv {stem}.anat/{image_type}_biascorr_brain_mask.nii.gz {stem}_brain-mask{suffix}.nii.gz\\n\"\n",
    "    \n",
    "    if skullstrip in ['synthstrip', 'both']:\n",
    "        #suffix = '_synthstrip' if skullstrip == 'both' else ''\n",
    "        suffix = '_synthstrip'\n",
    "        out_file = f\"{stem}_SkullStripped{suffix}.nii.gz\"\n",
    "        out_mask = f\"{stem}_brain-mask{suffix}.nii.gz\"\n",
    "        cmd += f\"mri_synthstrip -i {input_image} -o {out_file} -m {out_mask}\\n\"\n",
    "\n",
    "        \n",
    "    # Run fslmaths\n",
    "    cmd += f\"fslmaths {stem}.nii.gz {stem}.nii.gz -odt short\\n\"\n",
    "    \n",
    "    if clean_up == True:\n",
    "        cmd += f\"rm -r {stem}.anat\\n\"\n",
    "        \n",
    "    return cmd\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fa154f-c056-4d37-97a5-8bb54df31aa2",
   "metadata": {},
   "source": [
    "### Run Part 2; will submit jobs to SLURM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f8e54706-7492-4780-a34a-5ff07f65b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "skullstrip='synthstrip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "b7a8da70-3ef1-4ca7-8a1e-b86819b7c917",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Processing 7901-01-001: scan01 ***\n",
      "t1\n",
      "Reslicing /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-001/scan01/SELECTED/7901-01-001_scan01_t1_SkullStripped_synthstrip.nii.gz\n",
      "Bias Correcting /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-001/scan01/SELECTED/7901-01-001_scan01_t1_SkullStripped_synthstrip.nii.gz\n",
      "*** Processing 7901-01-001: scan03 ***\n",
      "t1\n",
      "Reslicing /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-001/scan03/SELECTED/7901-01-001_scan03_t1.nii.gz\n",
      "Bias Correcting /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-001/scan03/SELECTED/7901-01-001_scan03_t1.nii.gz\n",
      "*** Processing 7901-01-001: scan02 ***\n",
      "t1\n",
      "Reslicing /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-001/scan02/SELECTED/7901-01-001_scan02_t1.nii.gz\n",
      "Bias Correcting /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-001/scan02/SELECTED/7901-01-001_scan02_t1.nii.gz\n",
      "*** Processing 7901-01-002: scan01 ***\n",
      "t1\n",
      "Reslicing /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-002/scan01/SELECTED/7901-01-002_scan01_t1.nii.gz\n",
      "Bias Correcting /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-002/scan01/SELECTED/7901-01-002_scan01_t1.nii.gz\n",
      "*** Processing 7901-01-002: scan02 ***\n",
      "t1\n",
      "Reslicing /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-002/scan02/SELECTED/7901-01-002_scan02_t1.nii.gz\n",
      "Bias Correcting /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-002/scan02/SELECTED/7901-01-002_scan02_t1.nii.gz\n",
      "You have 5 Bias Correction Jobs submitted to SLURM; subject and job IDs are stored in bias_corr_dict\n",
      "You can type \"squeue -u $USER\" into your terminal to track SLURM job progress\n",
      "You can check the output file matching the jobid in bias_corr_dict to see code outputs and any errors\n"
     ]
    }
   ],
   "source": [
    "bias_corr_dict={}\n",
    "for subject, sessions in subject_sessions.items():\n",
    "    reg_target=None\n",
    "    subject_folder = os.path.join(output_dir, subject) \n",
    "    for session in sessions:\n",
    "        print(f'*** Processing {subject}: {session} ***')\n",
    "        session_folder=os.path.join(subject_folder, session)\n",
    "        \n",
    "        reg_target=set_registration_target(glob(f'{session_folder}/SELECTED/*.nii*'))\n",
    "        print(reg_target)\n",
    "        \n",
    "        #reg_image=glob(f'{session_folder}/SELECTED/*{reg_target}*')[0]\n",
    "        reg_image=f'{session_folder}/SELECTED/{subject}_{session}_{reg_target}.nii.gz'\n",
    "        reg_image = [file for file in glob(f'{session_folder}/SELECTED/*{reg_target}*') if 'orig' not in file and 'optibet' not in file and 'anat' not in file][0]\n",
    "        \n",
    "        print(f'Reslicing {reg_image}')\n",
    "        #reslice_image(reg_image)\n",
    "        \n",
    "        print(f'Bias Correcting {reg_image}')\n",
    "        if '1' in reg_target:\n",
    "            img_type='T1'\n",
    "        elif '2' in reg_target:\n",
    "            img_type='T2'\n",
    "        \n",
    "        command=bias_corr(reg_image, img_type, skullstrip=skullstrip, clean_up=False) \n",
    "        job_name = f\"bias_correct_{subject}_{session}_{reg_target}\"\n",
    "        # script_file=command_script(job_name, command)\n",
    "        # print(script_file)\n",
    "        # job_id=sbatch_script(job_name, script_file)\n",
    "        job_id=submit_slurm_job_test(job_name, command)\n",
    "        bias_corr_dict[(subject,session)] = job_id\n",
    "\n",
    "if len(bias_corr_dict) > 0:\n",
    "    print('You have', len(bias_corr_dict), 'Bias Correction Jobs submitted to SLURM; subject and job IDs are stored in bias_corr_dict')\n",
    "    print('You can type \"squeue -u $USER\" into your terminal to track SLURM job progress')\n",
    "    print('You can check the output file matching the jobid in bias_corr_dict to see code outputs and any errors')\n",
    "else:\n",
    "    print('You have', len(bias_corr_dict), 'Bias Correction Jobs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "d4b8e746-8254-4c80-8c81-ba5169729556",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('7901-01-001', 'scan01'): '1590448'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bias_corr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d7c59d25-8754-4a8e-813a-d92e9a3f0bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           1590695 bch-compu slurm_sc ch236393  R       3:22      1 compute-5-0-1\n",
      "           1587644 bch-inter     bash ch236393  R    3:30:11      1 compute-5-0-3\n",
      "           1590111 bch-inter     bash ch236393  R    1:22:00      1 compute-10-9\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fdb466-f65c-406e-a5ed-11d91b87103c",
   "metadata": {},
   "source": [
    "# Part 3: Co-Register and Skull Strip Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c69638ce-7024-4cef-9e5d-d37ca90cd618",
   "metadata": {},
   "source": [
    "- If you selected 'both' for skullstrip previously, please look at the Part 2 outputs and select your preferred method before running this section; you can use the below cell to update this if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1b4bc7-a9b8-4bf8-b5d6-73ea65380c2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "skullstrip='synthstrip' #['optibet', 'synthstrip', None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9522ce50-96ad-42e8-ae95-9989600acc5e",
   "metadata": {},
   "source": [
    "## Part 3 Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "6d3f6181-5a32-40ad-b715-46afb4a61659",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Co-register files to the Registration Target\n",
    "def co_register(working_dir, reg_image, moving_image, brain_mask=None, clean_up=True):\n",
    "    \n",
    "    \"\"\"\n",
    "    Co-registers a moving image to a registration target.\n",
    "\n",
    "    Parameters:\n",
    "    working_dir (str): The working directory.\n",
    "    reg_image (str): The registration target image.\n",
    "    moving_image (str): The moving image.\n",
    "    skullstrip (bool): Whether to skullstrip the image. Default is True.\n",
    "    clean_up (bool): Whether to clean up temporary files. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    cmd (str): The command to run.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(f'{working_dir}/warps'):\n",
    "        os.makedirs(f'{working_dir}/warps')\n",
    "    \n",
    "    moving_stem=os.path.basename(moving_image).split('.')[0]\n",
    "    \n",
    "    if os.path.exists(f\"{working_dir}/{moving_stem}_space-{reg_target}.nii.gz\"):\n",
    "        print(f\"WARNING: Input image file {moving_stem}_space-{reg_target}.nii.gz already exists. Skipping co-registration\")\n",
    "        return \n",
    "    \n",
    "    \n",
    "    cmd = f\"antsRegistrationSyNQuick.sh -d 3 -m {moving_image} -f {reg_image} -t sr -o {working_dir}/warps/{moving_stem}_space-{reg_target}\\n\"\n",
    "    \n",
    "    cmd +=f\"mv {working_dir}/warps/{moving_stem}_space-{reg_target}Warped.nii.gz {working_dir}/{moving_stem}_space-{reg_target}.nii.gz\\n\"\n",
    "    \n",
    "    if clean_up == True:\n",
    "        cmd +=f\" rm -r {working_dir}/warps\\n\"\n",
    "    \n",
    "    if brain_mask:\n",
    "        cmd += f\"fslmaths {brain_mask} -mul {working_dir}/{moving_stem}_space-{reg_target}.nii.gz {working_dir}/{moving_stem}_space-{reg_target}_SkullStripped.nii.gz\"\n",
    "                        \n",
    "    return cmd\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43621f5a-8539-4db6-a583-52e17cb70fa7",
   "metadata": {},
   "source": [
    "### Run Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3d0ad644-0287-402d-b403-7468ee14eb9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Processing 7901-01-001: scan01 ***\n",
      "t1\n",
      "Registering Images to the t1\n",
      "Registering /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-001/scan01/SELECTED/7901-01-001_scan01_t2.nii.gz\n",
      "*** Processing 7901-01-001: scan03 ***\n",
      "t1\n",
      "Registering Images to the t1\n",
      "Registering /lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/output_RDCRN_test_20240619/7901-01-001/scan03/SELECTED/7901-01-001_scan03_t2.nii.gz\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1650123/44989250.py\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m                     \u001b[0mcommand\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mco_register\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{session_folder}/COREGISTERED'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreg_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbrain_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclean_up\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m                     \u001b[0mjob_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"co-register_{subject}_{session}_{reg_target}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m                     \u001b[0mjob_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubmit_slurm_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m                     \u001b[0mco_reg_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1650123/1494043361.py\u001b[0m in \u001b[0;36msubmit_slurm_job\u001b[0;34m(job_name, command, partition, nodes, ntasks, cpus, mem, time)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mjob_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m     \u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mjob_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if skullstrip not in ['optibet', 'synthstrip', None]:\n",
    "    raise ValueError(\"Please change your skullstrip variable to either 'optibet', 'synthstrip', or None\")\n",
    "\n",
    "co_reg_dict={}\n",
    "for subject, sessions in subject_sessions.items():\n",
    "    reg_target=None\n",
    "    subject_folder = os.path.join(output_dir, subject) \n",
    "    for session in sessions:\n",
    "        print(f'*** Processing {subject}: {session} ***')\n",
    "        session_folder=os.path.join(subject_folder, session)\n",
    "        \n",
    "        reg_target=set_registration_target(glob(f'{session_folder}/SELECTED/*.nii*'))\n",
    "        print(reg_target)\n",
    "        \n",
    "        print(f'Registering Images to the {reg_target}') #~5 min per file\n",
    "        \n",
    "        if not os.path.exists(f'{session_folder}/COREGISTERED'):\n",
    "            os.makedirs(f'{session_folder}/COREGISTERED')\n",
    "            \n",
    "        reg_file=f'{session_folder}/SELECTED/{subject}_{session}_{reg_target}.nii.gz'\n",
    "        shutil.copy(reg_file, f'{session_folder}/COREGISTERED')\n",
    "        \n",
    "\n",
    "        brain_mask=None\n",
    "        if skullstrip != None:\n",
    "            suffix = '_' + skullstrip\n",
    "            brain_mask=f'{session_folder}/SELECTED/*brain-mask*{suffix}*'\n",
    "        \n",
    "        for file in glob(f'{session_folder}/SELECTED/*.nii*'): #this will get other files like _orig etc. \n",
    "                if reg_target not in file:\n",
    "                    print(f'Registering {file}')\n",
    "                    command=co_register(f'{session_folder}/COREGISTERED', reg_file, file, brain_mask, clean_up=False)    \n",
    "                    job_name = f\"co-register_{subject}_{session}_{reg_target}\"\n",
    "                    job_id=submit_slurm_job(job_name, command)\n",
    "                    co_reg_dict[(subject,session)] = job_id\n",
    "\n",
    "if len(co_reg_dict) > 0:\n",
    "    print('You have', len(co_reg_dict), 'Co-Registration Jobs submitted to SLURM; subject and job IDs are stored in the co_reg_dict')\n",
    "    print('You can type \"squeue -u $USER\" into your terminal  or \"!squeue -u $USER\" in a cell to track SLURM job progress')\n",
    "    print('You can check the output file matching the jobid in co_reg_dict to see code outputs and any errors')\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "b22074b2-2c2f-4eb2-b3fb-f5745a1e8e87",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('7901-01-001', 'scan01'): '1591012',\n",
       " ('7901-01-001', 'scan03'): '1591013',\n",
       " ('7901-01-001', 'scan02'): '1591014',\n",
       " ('7901-01-002', 'scan01'): '1591015',\n",
       " ('7901-01-002', 'scan02'): '1591016'}"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "co_reg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "349d1508-dc48-4268-88f5-01f4f6cc1946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           1587644 bch-inter     bash ch236393  R    4:15:06      1 compute-5-0-3\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "032a52a8-0c65-4036-bfed-00e8f51f69fd",
   "metadata": {},
   "source": [
    "# Part 4: Register to MNI\n",
    "- If you are planning to trace lesions, do that first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d99924-61eb-4bcc-abad-649241cd904b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nilearn.datasets.load_mni152_template() #loads T1w"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63f375b-00d2-4942-ae83-9a74765aad15",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 4 option a: EasyReg\n",
    "- 1-5 minute runtime, pre-creating the segmentations using the \"robust\" setting likely gives better results for large lesions\n",
    "- Can register across modality (i.e. T2w to T1w)\n",
    "- Citation: Iglesias, J. E. (2023). A ready-to-use machine learning tool for symmetric multi-modality registration of brain MRI. Scientific Reports, 13(1), Article 1. https://doi.org/10.1038/s41598-023-33781-0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "3a272b79-8617-47bd-9159-98ac0ae8e064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def easy_reg(working_dir, source_brain, target_brain, lesion_mask=None, other_brains=[], synthseg_robust=True):\n",
    "\n",
    "    source_name=os.path.basename(source_brain).split('.')[0]\n",
    "\n",
    "    cmd = f'echo Running EasyReg for {source_brain}\\n'\n",
    "    cmd += ' source activate easyreg\\n'\n",
    "    cmd += 'LD_LIBRARY_PATH=/opt/ohpc/pub/mpi/openmpi3-gnu8/3.1.4/lib:/opt/ohpc/pub/compiler/gcc/8.3.0/lib64\\n'\n",
    "    cmd += 'CUDNN_PATH=$(dirname $(python -c \"import nvidia.cudnn;print(nvidia.cudnn.__file__)\"))\\n'\n",
    "    cmd += 'export LD_LIBRARY_PATH=$CUDNN_PATH/lib:/lab-share/Neuro-Cohen-e2/Public/environment/conda/easyreg/lib/python3.9/site-packages/tensorrt_libs/:$LD_LIBRARY_PATH:\\n'\n",
    "\n",
    "    if synthseg_robust==True:\n",
    "        cmd +=f'mri_synthseg --i {source_brain} --o {working_dir}/{source_name}_synthseg.nii.gz --parc --robust\\n'\n",
    "        \n",
    "    cmd += ' '.join([\n",
    "        'mri_easyreg',\n",
    "        '--ref', target_brain,\n",
    "        '--flo', source_brain,\n",
    "        '--ref_seg', f'0_pipeline_scripts/mni_icbm152_t1_tal_nlin_asym_09c_brain_synthseg.nii.gz',\n",
    "        '--flo_seg', f'{working_dir}/{source_name}_synthseg.nii.gz',\n",
    "        '--flo_reg', f'{working_dir}/{source_name}_MNI.nii.gz',\n",
    "        '--fwd_field', f'{working_dir}/{source_name}_to_MNI_warp.nii.gz'\n",
    "    ])\n",
    "    \n",
    "    if lesion_mask:\n",
    "        cmd +=f'mri_easywarp --i {lesion_mask} --o {working_dir}/{source_name}_lesion_MNI.nii.gz --field {working_dir}/{source_name}_to_MNI_warp.nii.gz --nearest'\n",
    "             \n",
    "    if other_brains:\n",
    "        for brain in other_brains:\n",
    "            brain_name=os.path.basename(brain).split('.')[0]\n",
    "            cmd +=f'mri_easywarp --i {brain} --o {working_dir}/{brain_name}_MNI.nii.gz --field {working_dir}/{source_name}_to_MNI_warp.nii.gz'\n",
    "\n",
    "    return cmd\n",
    "             "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76faf7d9-aa05-4210-bced-cd51dfe46454",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Part 4 option b: ANTs SyN Regular or Quick\n",
    "- Regular: 1-2 hour runtime, may do better than easyReg for brains with larger lesions/lots of deformation\n",
    "- Quick: 5-15 min run time, but ANTs regular has significantly better results for lesioned brains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "3dba0948-50a1-4cb3-a43e-8a74f03b5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ants(working_dir, source_brain, target_brain, lesion_mask=None, other_brains=[], transform='s', histogram_matching=False, quick=False):\n",
    "    \n",
    "    \"\"\"\n",
    "    Co-registers a moving image to a registration target.\n",
    "\n",
    "    Parameters:\n",
    "    working_dir (str): The working directory.\n",
    "    reg_image (str): The registration target image.\n",
    "    moving_image (str): The moving image.\n",
    "    skullstrip (bool): Whether to skullstrip the image. Default is True.\n",
    "    clean_up (bool): Whether to clean up temporary files. Default is True.\n",
    "\n",
    "    Returns:\n",
    "    cmd (str): The command to run.\n",
    "    \"\"\"\n",
    "    \n",
    "    if not os.path.exists(f'{working_dir}/warps'):\n",
    "        os.makedirs(f'{working_dir}/warps')\n",
    "    \n",
    "    source_stem=os.path.basename(source_brain).split('.')[0]\n",
    "    \n",
    "    if os.path.exists(f\"{working_dir}/{source_stem}_MNI.nii.gz\"):\n",
    "        print(f\"WARNING: Input image file {source_stem}_MNI.nii.gz already exists. Skipping...\")\n",
    "        return \n",
    "    \n",
    "    lesion_mask=''\n",
    "    if lesion_mask:\n",
    "        add_lesion_mask=f'-x {lesion_mask}'\n",
    "    \n",
    "    add_hist_match=''\n",
    "    if histogram_matching == True:\n",
    "        add_hist_match='-j 1'\n",
    "    \n",
    "    #The moving and fixed image are switched so that the lesion mask can be used in the registration\n",
    "    # Usually the fixed image, aka target, would be the MNI brain \n",
    "    ants_cmd='antsRegistrationSyN.sh'\n",
    "    if quick:\n",
    "        ants_cmd='antsRegistrationSyNQuick.sh'\n",
    "    \n",
    "    cmd =\"source /lab-share/Neuro-Cohen-e2/Public/environment/load_neuroimaging_env.sh\\n\"\"\n",
    "    cmd += f\"{ants_cmd} -d 3 -m {target_brain} -f {source_brain} -t {transform} {add_lesion_mask} {add_hist_match} -o {working_dir}/warps/{source_stem}_MNI_\\n\"\n",
    "    \n",
    "    cmd +=f\"mv {working_dir}/warps/{source_stem}_MNI_InverseWarped.nii.gz {working_dir}/{source_stem}_MNI.nii.gz\\n\"\n",
    "    \n",
    "    if lesion_mask:\n",
    "        lesion_stem=lesion_mask.split('.')[0]\n",
    "        lesion_cmd = [\n",
    "            'antsApplyTransforms', \n",
    "            '-d', '3', \n",
    "            '-i', f'{lesion_mask}', \n",
    "            '-r', f'{target_brain}', \n",
    "            '-t', f'[{working_dir}/warps/{source_stem}_MNI_0GenericAffine.mat, 1]', \n",
    "            '-t', f'{source_stem}_MNI_1InverseWarp.nii.gz', \n",
    "            '-n', 'NearestNeighbor', \n",
    "            '-o', f'{lesion_stem}_MNI.nii.gz'\n",
    "        ]\n",
    "        \n",
    "        cmd += ' '.join(lesion_cmd) + '\\n'\n",
    "\n",
    "    if other_brains:\n",
    "        for brain in other_brains:\n",
    "            brain_stem=os.path.basename(brain).split('.')[0]\n",
    "            brain_cmd += [\n",
    "                'antsApplyTransforms', \n",
    "                '-d', '3', \n",
    "                '-i', f'{lesion_mask}', \n",
    "                '-r', f'{target_brain}', \n",
    "                '-t', f'[{working_dir}/warps/{source_stem}_MNI_0GenericAffine.mat, 1]', \n",
    "                '-t', f'{source_stem}_MNI_1InverseWarp.nii.gz', \n",
    "                '-n', 'Linear', \n",
    "                '-o', f'{brain_stem}_MNI.nii.gz'\n",
    "            ]\n",
    "        \n",
    "        cmd += ' '.join(brain_cmd) + '\\n'\n",
    "    \n",
    "                         \n",
    "    return cmd\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd440996-f79e-48a6-8fd6-eeb553e06b3d",
   "metadata": {},
   "source": [
    "### Run Part 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "04f96799-e385-42b8-a389-b251a4880a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Processing c001: s01 ***\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No registration target found in []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1650123/2394794566.py\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0msession_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mreg_target\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mset_registration_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{session_folder}/COREGISTERED/*.nii*'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreg_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1650123/2763854969.py\u001b[0m in \u001b[0;36mset_registration_target\u001b[0;34m(file_names)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreg_target\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"No registration target found in {file_names}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mreg_target\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: No registration target found in []"
     ]
    }
   ],
   "source": [
    "mni_reg_dict={}\n",
    "for subject, sessions in subject_sessions.items():\n",
    "    reg_target=None\n",
    "    subject_folder = os.path.join(output_dir, subject) \n",
    "    for session in sessions:\n",
    "        print(f'*** Processing {subject}: {session} ***')\n",
    "        session_folder=os.path.join(subject_folder, session)\n",
    "        \n",
    "        reg_target=set_registration_target(glob(f'{session_folder}/COREGISTERED/*.nii*'))\n",
    "        print(reg_target)\n",
    "        \n",
    "        if '1' in reg_target:\n",
    "            target_image=f'/lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/0_pipeline_scripts/mni_icbm152_t1_tal_nlin_asym_09c_brain.nii.gz'\n",
    "        elif '2' in reg_target:\n",
    "            target_image=f'/lab-share/Neuro-Cohen-e2/Public/notebooks/gmiller/Pipeline/0_pipeline_scripts/mni_icbm152_t2_tal_nlin_asym_09c_brain.nii.gz'\n",
    "        \n",
    "\n",
    "        if not os.path.exists(f'{session_folder}/MNI_SPACE'):\n",
    "            os.makedirs(f'{session_folder}/MNI_SPACE')\n",
    "            \n",
    "        source_file=f'{session_folder}/COREGISTERED/{subject}_{session}_{reg_target}.nii.gz'\n",
    "        \n",
    "        #look for lesion mask\n",
    "        lesion_mask_files=glob(f'{session_folder}/COREGISTERED/*lesion*nii*')\n",
    "        if lesion_mask_files:\n",
    "            lesion_mask = lesion_mask_files[0]\n",
    "        if len(lesion_mask_files) > 1:\n",
    "            print(f\"Warning: Multiple lesion mask files found. Using the first one: {lesion_mask}\")\n",
    "        else:\n",
    "            lesion_mask = None\n",
    "            print(\"No lesion mask file found.\")\n",
    "\n",
    "        #look for other brains to warp\n",
    "        other_brains=glob(f'{session_folder}/COREGISTERED/*space-{reg_target}*')\n",
    "        if other_brains:\n",
    "            print(\"Found\", len(other_brains), \"to register to MNI space\")\n",
    "        else:\n",
    "            print(\"Found no other brains to register to MNI space\")\n",
    "        \n",
    "        print(f'Registering the {reg_target} to MNI using {mni_software}') \n",
    "        if mni_software == 'EasyReg':\n",
    "            command=easy_reg(f'{session_folder}/MNI_SPACE', source_file, target_image, lesion_mask, other_brains, synthseg_robust=False)\n",
    "        elif mni_software == 'ANTs':\n",
    "            command=ants(working_dir, source_file, target_image, lesion_mask, other_brains, transform='s', histogram_matching=False, quick=False)\n",
    "        elif mni_software == 'ANTsQuick':\n",
    "            command=ants(working_dir, source_file, target_image, lesion_mask, other_brains, transform='s', histogram_matching=False, quick=True)\n",
    "        else:\n",
    "            print(\"Please set mni_software to 'EasyReg', 'ANTs', or 'ANTsQuick'\")\n",
    "            \n",
    "        job_name = f\"MNI-register_{subject}_{session}_{reg_target}\"\n",
    "        job_id=submit_slurm_job(job_name, command)\n",
    "        mni_reg_dict[(subject,session)] = job_id\n",
    "        \n",
    "\n",
    "if len(mni_reg_dict) > 0:\n",
    "    print('You have', len(mni_reg_dict), 'MNI Registration Jobs submitted to SLURM; subject and job IDs are stored in the co_reg_dict')\n",
    "    print('You can type \"squeue -u $USER\" into your terminal  or \"!squeue -u $USER\" in a cell to track SLURM job progress')\n",
    "    print('You can check the output file matching the jobid in co_reg_dict to see code outputs and any errors')\n",
    "                                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "db61f3c7-8304-4418-b66c-a4be81aa9978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('c001', 's01'): '1317966'}"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mni_reg_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "6ad9f6d4-7419-4e67-9e01-147fdce2061c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "           1317128 bch-compu tmpz_4j_ ch236393  R      59:01      1 compute-5-0-3\n",
      "           1317966 bch-compu tmp3njgj ch236393  R       0:41      1 compute-5-0-0\n",
      "           1313822 bch-inter     bash ch236393  R    5:57:22      1 compute-10-9\n",
      "           1316284 bch-inter     bash ch236393  R    2:12:42      1 compute-5-0-3\n"
     ]
    }
   ],
   "source": [
    "!squeue -u $USER"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3.10 (nimlab)",
   "language": "python",
   "name": "nimlab_py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
